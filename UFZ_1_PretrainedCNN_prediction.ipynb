{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UFZ_1_PretrainedCNN_prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanbseo/FlickrWorkshopUFZ/blob/master/UFZ_1_PretrainedCNN_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "58UhYEmk43rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this session, we are using a simple photo tagging without additional training. The model we are going to use is one of the best Convolutional Neural Network (CNN) model. It is generally well trained about intermediate level features like color, texture, shape. However, it does not know well about ecosystem and ecological objects.   \n",
        "\n",
        "Note: don't forget to change the runtime type from None to GPU!"
      ]
    },
    {
      "metadata": {
        "id": "rLVIdfS-76Hw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First some preparation scripts \n",
        "# I'll skip this part today, but please let me know if you want to get to know about those \n",
        " \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import skimage.io # scikit image \n",
        "\n",
        "### The below is to avoid certificat error \n",
        "# source: https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error\n",
        "import requests\n",
        "\n",
        "requests.packages.urllib3.disable_warnings()\n",
        "\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
        "    pass\n",
        "else:\n",
        "    # Handle target environment that doesn't support HTTPS verification\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zEvbR9NK9dYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import Keras and image manipulating libraries \n",
        "\n",
        "from keras.applications import inception_resnet_v2\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "from keras import backend as k\n",
        "\n",
        "\n",
        "# # Typical input image sizes to a Convolutional Neural Network trained on ImageNet are 224×224, 227×227, 256×256, and 299×299; however, you may see other dimensions as well.\n",
        "# # VGG16, VGG19, and ResNet all accept 224×224 input images while Inception V3 and Xception require 299×299 pixel inputs, as demonstrated by the following code block:\n",
        "# # Nasnet large 331x331. If you train your own classes (i.e. the final Fully Connected/Dense layer), you can modify the img size \n",
        "img_width, img_height = 299, 299\n",
        "\n",
        "# to decode predictions\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZVBtH4xrDLnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import urllib\n",
        "# import keras \n",
        "# image = urllib.request.urlopen(image_url)\n",
        "# content = image.read()\n",
        "# keras.preprocessing.image.load_img(content, target_size=(img_height, img_width))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tkf1zeSLAJyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use a model called `inceptionv4resnetv2`. It is an excellent CNN model extensively used in computer vision. In standardides evaluation set-up, it is said it has 80% Top-1 Accuracy and 95% Top-5 Accuracy (i.e.  if the target label is one of your top 5 predictions)."
      ]
    },
    {
      "metadata": {
        "id": "39k2Qx5q9vJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Load the inception_v4_resnet_v2\n",
        "# https://ai.googleblog.com/2016/08/improving-inception-and-image.html\n",
        "model_pretrained = inception_resnet_v2.InceptionResNetV2(weights='imagenet')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7ovBdy1-a0c",
        "colab_type": "code",
        "outputId": "017976ee-4537-468e-ffab-c6fa7f90f612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load a image from Internet\n",
        "image_url = \"http://www.sportstimemagazine.com/wp-content/uploads/2017/06/Boost_18_1112_29-752x440.jpg\"\n",
        "image_url = \"https://images.mentalfloss.com/sites/default/files/styles/mf_image_16x9/public/516438-istock-637689912.jpg\"\n",
        "# image_url = \"https://boygeniusreport.files.wordpress.com/2016/11/puppy-dog.jpg?quality=98&strip=all\"\n",
        "\n",
        "# original = skimage.io.imread(image_url)\n",
        "# skimage.io.imshow(original)\n",
        "\n",
        "# from skimage.transform import resize\n",
        "# image_input = resize(original, (img_height, img_width))\n",
        "\n",
        "# skimage.io.imshow(image_input)\n",
        "\n",
        "# # load an image in PIL format\n",
        "# # original = load_img(filename, target_size=(299, 299))\n",
        "# original = load_img(image_url, target_size=(331, 331)) @todo did not work scikit + PIL.. need to fix\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PIL image size (299, 299)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XOmyklkjAOWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some necessary image pre-processing. Basically we read an image and convert the image to numpy format and put it into a batch array; in thie example we have a single image but many will be if batch processing."
      ]
    },
    {
      "metadata": {
        "id": "amHrUBeu-0OI",
        "colab_type": "code",
        "outputId": "2fc3c295-81e1-4488-9714-c16dffe23d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from PIL import Image\n",
        " \n",
        "from urllib.request import urlopen\n",
        "img =Image.open(urlopen(image_url))\n",
        "image_input =img.resize((img_height,img_width),Image.ANTIALIAS)\n",
        "# image_input.save('url.jpg','jpeg') # to save the image locally \n",
        "\n",
        "print('PIL image size', image_input.size)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert the PIL image to a numpy array\n",
        "# IN PIL - image is in (width, height, channel)\n",
        "# In Numpy - image is in (height, width, channel)\n",
        "numpy_image = img_to_array(image_input)\n",
        "# plt.imshow(np.uint8(numpy_image))\n",
        "# plt.show()\n",
        "print('numpy array size',numpy_image.shape)\n",
        "\n",
        "# Convert the image / images into batch format\n",
        "# expand_dims will add an extra dimension to the data at a particular axis\n",
        "# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
        "# Thus we add the extra dimension to the axis 0.\n",
        "image_batch = np.expand_dims(numpy_image, axis=0)\n",
        "print('image batch size', image_batch.shape)\n",
        "# plt.imshow(np.uint8(image_batch[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy array size (299, 299, 3)\n",
            "image batch size (1, 299, 299, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vcrXtqFtAwZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before looking into the network or anything, first we are doing something interesting and consider the inception model as a magic black-box. "
      ]
    },
    {
      "metadata": {
        "id": "BhZKVJwFAv3m",
        "colab_type": "code",
        "outputId": "7182b6ae-4782-4cb4-cca6-dba1be1b8238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "# get the predicted probabilities for each class\n",
        "predictions = model_pretrained.predict(processed_image)\n",
        "# print predictions\n",
        "\n",
        "# print(predictions)\n",
        "\n",
        "# convert the probabilities to class labels\n",
        "# We will get top 5 predictions which is the default\n",
        "predicted_tags = decode_predictions(predictions, top=5)[0]\n",
        "\n",
        "# print predictions\n",
        "\n",
        "print('Predicted:', predicted_tags )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted: [('n02099601', 'golden_retriever', 0.9275075), ('n02099712', 'Labrador_retriever', 0.0100292), ('n02102318', 'cocker_spaniel', 0.0013605809), ('n02101556', 'clumber', 0.001314079), ('n02093991', 'Irish_terrier', 0.0011871092)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-A_0niCeTB_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is the dominant entry in the prediction vector\n",
        "dominant_feature_idx = np.argmax(predictions[0])\n",
        "dominant_output = model_pretrained.output[:, dominant_feature_idx]\n",
        "\n",
        " \n",
        "# The is the output feature map of the `conv_7b_ac` layer,\n",
        "# the last convolutional layer in InceptionResnetV2\n",
        "last_conv_layer = model_pretrained.get_layer('conv_7b_ac')\n",
        "\n",
        "# This is the gradient of the dominant class with regard to\n",
        "# the output feature map of `conv_7b_ac`\n",
        "grads = k.gradients(dominant_output, last_conv_layer.output)[0]\n",
        "\n",
        "# This is a vector of shape (1536,), where each entry\n",
        "# is the mean intensity of the gradient over a specific feature map channel\n",
        "pooled_grads = k.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "# This function allows us to access the values of the quantities we just defined:\n",
        "# `pooled_grads` and the output feature map of `conv_7b_ac`,\n",
        "# given a sample image\n",
        "iterate = k.function([model_pretrained.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "# These are the values of these two quantities, as Numpy arrays,\n",
        "# given our sample image of two elephants\n",
        "pooled_grads_value, conv_layer_output_value = iterate([processed_image])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjrStAk0TnJ6",
        "colab_type": "code",
        "outputId": "5ffdc9e7-64f9-44fd-d52f-d84465a521af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "grad_dim = grads.shape[3]\n",
        "print(grad_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e1ZJO3CRTc5e",
        "colab_type": "code",
        "outputId": "0e7835bc-6d3f-4eb3-e056-69ab50345cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "cell_type": "code",
      "source": [
        "# We multiply each channel in the feature map array\n",
        "# by \"how important this channel is\" with regard to the elephant class\n",
        "for i in range(grad_dim):\n",
        "  conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "# The channel-wise mean of the resulting feature map\n",
        "# is our heatmap of class activation\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "\n",
        "\n",
        "# For visualization purpose, we will also normalize the heatmap between 0 and 1:\n",
        "\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAFSCAYAAAB2cI2KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEE0AMCCaAGBANAHA4H8boPz5If7vMgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 396x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "s1EHO7VYEBgv",
        "colab_type": "code",
        "outputId": "5414dd9b-ec4b-417c-dbb6-4453a1037791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# We use cv2 to load the original image\n",
        "img = cv2.imread(fname)\n",
        "\n",
        "# We resize the heatmap to have the same size as the original image\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# We convert the heatmap to RGB\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# We apply the heatmap to the original image\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "# 0.4 here is a heatmap intensity factor\n",
        "superimposed_img = heatmap * 0.4 + img\n",
        "\n",
        "# Choose a font\n",
        "font = cv2.FONT_HERSHEY_COMPLEX\n",
        "\n",
        "# Draw the text\n",
        "\n",
        "for x in range(0, num_classes):\n",
        "  tags = classes[x] + \":\" + str((predictions[0][x]*100).round(2)) + \"%\"\n",
        "  cv2.putText(superimposed_img,tags,(10, 100 * x + int(img.shape[0]/3)), font, 2,(255,255,255),2) # ,cv2.LINE_AA)\n",
        "\n",
        "\n",
        "# os.mkdir(\"Result/Heatmap_\" + modelname + \"/\" + predicted_class)\n",
        "\n",
        "# Save the image to disk\n",
        "cv2.imwrite(\"Result/Heatmap_\" + modelname + \"/\" + predicted_class + \"/AttentionMap_\" + predicted_class + \"_\" + filename, superimposed_img)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-30eea2591537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The is the output feature map of the `conv_7b_ac` layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the last convolutional layer in InceptionResnetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlast_conv_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_7b_ac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This is the gradient of the dominant class with regard to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_trained' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZUXS3vG72Tm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "When you are done with the exercise, we recommend continue exploring the python notebook of François Chollet (https://github.com/fchollet/deep-learning-with-python-notebooks), who developed the libaray we use, Keras. He authored a book called `Deep Learning with Python' (https://www.manning.com/books/deep-learning-with-python); the book is the best introduction to the topic as far as I explored.\n",
        "\n",
        "Recommend going through \n",
        "\n",
        "https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb\n",
        "https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "Ictv5zi540GX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}