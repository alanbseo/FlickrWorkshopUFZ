{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UFZ_3_PretrainedCNN_retraining.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanbseo/FlickrWorkshopUFZ/blob/master/UFZ_3_PretrainedCNN_retraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OLylcjOyqxzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First some preparation scripts \n",
        "# I'll skip this part today, but please let me know if you want to get to know about those \n",
        " \n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import skimage.io # scikit image \n",
        "\n",
        "### The below is to avoid certificat error \n",
        "# source: https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error\n",
        "import requests\n",
        "\n",
        "requests.packages.urllib3.disable_warnings()\n",
        "\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
        "    pass\n",
        "else:\n",
        "    # Handle target environment that doesn't support HTTPS verification\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCnYbs_Xr4aF",
        "colab_type": "code",
        "outputId": "b6aa92f5-83f0-43b7-aac7-45b0d6f9ef4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "# !rm -fr /content/FlickrWorkshopData # # if does not work.. \n",
        "\n",
        "!git clone https://github.com/alanbseo/FlickrWorkshopUFZ.git /content/FlickrWorkshopData\n",
        "  \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/FlickrWorkshopData'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (381/381), done.\u001b[K\n",
            "remote: Compressing objects: 100% (380/380), done.\u001b[K\n",
            "remote: Total 381 (delta 12), reused 360 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (381/381), 126.37 MiB | 47.88 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rbdeqwrTuxSV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "917ecb1c-9c2d-48a8-9f0f-4986128c38fb"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/FlickrWorkshopData': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Itq-p7nIq_ZK",
        "colab_type": "code",
        "outputId": "1d36b44f-a927-4a70-ce68-72f2bd23cfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import Keras and image manipulating libraries \n",
        "\n",
        "from keras.applications import inception_resnet_v2\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        " \n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        " \n",
        "\n",
        "import csv\n",
        "import pathlib\n",
        "import fnmatch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        " \n",
        "\n",
        "\n",
        "from keras import backend as k\n",
        "\n",
        "\n",
        "# to decode predictions\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "\n",
        " "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kWWnGk_1rBmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Typical input image sizes to a Convolutional Neural Network trained on ImageNet are 224×224, 227×227, 256×256, and 299×299; however, you may see other dimensions as well.\n",
        "# VGG16, VGG19, and ResNet all accept 224×224 input images while Inception V3 and Xception require 299×299 pixel inputs, as demonstrated by the following code block:\n",
        "# Nasnet large 331x331. If you train your own classes (i.e. the final Fully Connected/Dense layer), you can modify the img size \n",
        " img_width, img_height = 662, 662\n",
        "  \n",
        "train_data_dir = \"Photos_338_retraining/train\"\n",
        "validation_data_dir = \"Photos_338_retraining/validation\"\n",
        "\n",
        "nb_train_samples = 210\n",
        "nb_validation_samples = 99\n",
        "\n",
        "num_classes = 5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rkko05berGd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Fine-tune a neural network on a new set of classes\n",
        "\n",
        "The task of fine-tuning a network is to tweak the parameters of an already trained network so that it adapts to the new task at hand. As explained here, the initial layers learn very general features and as we go higher up the network, the layers tend to learn patterns more specific to the task it is being trained on. Thus, for fine-tuning, we want to keep the initial layers intact ( or freeze them ) and retrain the later layers for our task.\n",
        "Thus, fine-tuning avoids both the limitations discussed above.\n",
        "The amount of data required for training is not much because of two reasons. First, we are not training the entire network. Second, the part that is being trained is not trained from scratch.\n",
        "Since the parameters that need to be updated is less, the amount of time needed will also be less.\n",
        "\n",
        "\n",
        "\n",
        "References:\n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
        "https://gist.github.com/liudanking\n",
        "https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
        "https://github.com/jkjung-avt/keras-cats-dogs-tutorial/blob/master/train_inceptionresnetv2.py\n",
        "https://forums.fast.ai/t/globalaveragepooling2d-use/8358\n"
      ]
    },
    {
      "metadata": {
        "id": "04A59fbnrhbl",
        "colab_type": "code",
        "outputId": "f8fc6e51-630a-495b-e0c7-0289eebe8be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(os.listdir(\".\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VWpQYbUCrT3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45e4b1e4-3f76-4fba-9fc8-a5b987197b65"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(os.listdir(\".\"))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Saxony_Flickr_Selection', '.git', 'GoogleColab_setup.ipynb', 'UFZ_demo_M_RCNN_ver1.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gXiPo_gLsw_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "56a9972e-2c61-4dfa-e19a-7558157b1014"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "default_path = 'FlickrWorkshopData'\n",
        " \n",
        "os.chdir(default_path)\n",
        "# photo_path = default_path + '/Photos_168_retraining'\n",
        "\n",
        "\n",
        "from keras.applications import inception_resnet_v2\n",
        "\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import metrics\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as k\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3eda65d04763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdefault_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'FlickrWorkshopData'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# photo_path = default_path + '/Photos_168_retraining'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FlickrWorkshopData'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cSvrNrfrtvmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9a62f5ff-101d-48b2-f96e-11773a899222"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "##### build our classifier model based on pre-trained InceptionResNetV2:\n",
        "\n",
        "\n",
        "# Load the base pre-trained model\n",
        "\n",
        "# do not include the top fully-connected layer\n",
        "# 1. we don't include the top (fully connected) layers of InceptionResNetV2\n",
        "\n",
        "model = inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet',input_tensor=None, input_shape=(img_width, img_height, 3))\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the all layers.\n",
        "# i.e. freeze all InceptionV3 layers\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# New dataset is small and similar to original dataset:\n",
        "# There is a problem of over-fitting, if we try to train the entire network. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.\n",
        "# So lets freeze all the layers and train only the classifier\n",
        "\n",
        "# first: train only the top layers\n",
        "\n",
        "# for layer in net_final.layers[:FREEZE_LAYERS]:\n",
        "#     layer.trainable = False\n",
        "# for layer in net_final.layers[FREEZE_LAYERS:]:\n",
        "#     layer.trainable = True\n",
        "\n",
        "x = model.output\n",
        "\n",
        "# Now that we have set the trainable parameters of our base network, we would like to add a classifier on top of the convolutional base. We will simply add a fully connected layer followed by a softmax layer with num_classes outputs.\n",
        "\n",
        "# Adding custom Layer\n",
        "# x = Flatten()(x)\n",
        "# add a global spatial average pooling layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "\n",
        "# If the network is stuck at 50% accuracy, there’s no reason to do any dropout. Dropout is a regularization process to avoid overfitting. But your problem is underfitting.\n",
        "# x = Dropout(0.5)(x) # 50% dropout\n",
        "\n",
        "# A Dense (fully connected) layer which generates softmax class score for each class\n",
        "predictions = Dense(num_classes, activation='softmax', name='softmax')(x)\n",
        "\n",
        "\n",
        "\n",
        "# creating the final model\n",
        "# this is the model we will train\n",
        "model_final = Model(inputs = model.input, outputs = predictions)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9bcaff456257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# A Dense (fully connected) layer which generates softmax class score for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "spfWq81jt4H3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## load previously trained weights\n",
        "model_final.load_weights('TrainedWeights/InceptionResnetV2_Saxony_retrain_flickr_final_epoch100_acc0.99.h5')\n",
        "\n",
        "\n",
        "\n",
        "# Compile the final model using an Adam optimizer, with a low learning rate (since we are 'fine-tuning')\n",
        "# For classification, categorical_crossentropy is most often used. It measures the information between the predicted and the true class labels similarly with the mutual information. It is\n",
        "model_final.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(model_final.summary())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVzC1ZaCt5jp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initiate the train and test generators with data Augumentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\",\n",
        "    zoom_range = 0.3,\n",
        "    width_shift_range = 0.3,\n",
        "    height_shift_range=0.3,\n",
        "    rotation_range=30)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\",\n",
        "    zoom_range = 0.3,\n",
        "    width_shift_range = 0.3,\n",
        "    height_shift_range=0.3,\n",
        "    rotation_range=30)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = \"categorical\")\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    class_mode = \"categorical\")\n",
        "\n",
        "\n",
        "\n",
        "# show class indices\n",
        "print('****************')\n",
        "for cls, idx in train_generator.class_indices.items():\n",
        "    print('Class #{} = {}'.format(idx, cls))\n",
        "print('****************')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S0NJgzkgt7n9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Save the model according to the conditions\n",
        "checkpoint = ModelCheckpoint(\"TrainedWeights/InceptionResnetV2_Saxony_retrain.h5\", monitor='acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "# Setup the early stopping criteria\n",
        "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "\n",
        "\n",
        "# Re-train the model\n",
        "history = model_final.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples,\n",
        "    epochs = epochs,\n",
        " #   validation_data = validation_generator,\n",
        " #   validation_steps = nb_validation_samples,\n",
        "    callbacks = [checkpoint, early])\n",
        "\n",
        "# at this point, the top layers are well trained.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7L9X9AvQt-BX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model\n",
        "model_final.save('TrainedWeights/InceptionResnetV2_Saxony_retrain_flickr_final.h5')\n",
        "\n",
        "# Save the model architecture\n",
        "with open('InceptionResnetV2_Saxony_retrain_flickr_final_architecture.json', 'w') as f:\n",
        "    f.write(model_final.to_json())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}